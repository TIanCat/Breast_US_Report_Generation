# Development and Evaluation of AI System for Breast Ultrasound Report Generation: A Retrospective, Multicentre, Cohort Study

Jian Wang,
Jinfeng Xu,
Xin Yang,
Huaiyu Wu,
Xiliang Zhu,
Rusi Chen,
Ao Chang,
Yanlin Chen,
Haoran Dou,
Ruobing Huang,
Jun Cheng,
Yongsong Zhou,
Rui Gao,
Keen Yang,
Guoqiu Li,
Jing Chen,
Hongtian Tian,
Ning Gu,
Fajin Dong,
Dong Ni

**Introduction:** In women, breast cancer accounts for 31\% of malignant tumors and causes 15\% of cancer deaths. Early detection and diagnosis are crucial for successful treatment of breast cancer. Mammography and ultrasound (US) are the primary imaging modalities for breast cancer screening. However, for women with dense breast tissue, the sensitivity of mammography decreases from 85\% to 48–64\%. In contrast, US does not have this defect, and it is non-invasive, low-cost, and real-time. This makes US more suitable for women with dense breasts, especially those in developing countries. The National Health Commission of China has recommended US as the preferred imaging modality for breast cancer screening because Asian women tend to have higher breast density compared to Western women.

A breast US (BUS) report records the BUS examination results and consists of findings and impressions. Findings describe the observations of US images by radiologists based on the Breast Imaging Reporting and Data System (BI-RADS), and impressions describe diagnostic conclusions. There are several problems in report writing. First, the quality of the BUS report is easily affected by the experience level of the radiologist. Inexperienced radiologists may misdiagnose, resulting in low-quality reports. Second, the reproducibility of reports is insufficient. For identical images, reports written by different radiologists, or even reports written by the same radiologist at different times, may be inconsistent. Finally, writing BUS reports is time-consuming. A report may consist of hundreds of words, and writing such a report will reduce the efficiency of BUS examinations. These challenges are more pronounced in developing countries, where radiologists are less experienced, medical training is less standardized, and patient-to-radiologist ratios tend to be higher than in developed countries. Therefore, it is of great significance to develop a tool that can assist radiologists in writing BUS reports in developing countries.

In recent years, many artificial intelligence (AI) technologies have been proposed to solve various clinical challenges. For example, keyframe recognition, lesion detection, benign and malignant diagnosis and other related technologies have been widely explored on US images. In particular, many diagnostic techniques have been proven effective by clinical studies. Automated generation of medical reports may alleviate the above clinical problems. AI models have been developed for automatic report generation based on chest X-ray images. However, there are very few studies exploring such techniques on BUS images. In addition, the effectiveness of these techniques in the clinic has not yet been reported.
In this retrospective multicenter study, we aimed to develop and evaluate an AI system that automatically generates BUS reports from multi-modality and multi-view US images. The system was developed and quantitatively evaluated on a large-scale dataset (including over 100,000 cases) collected from the main center. Qualitative evaluations were performed on external test sets collected from two other centers. The core of the evaluation is to determine the diagnostic capabilities of the AI system and its value to radiologists.
![image](https://github.com/TIanCat/Breast_US_Report_Generation/blob/main/figure/framework.png)
The overall design of this study. (a) The developed system consists of the finding and impression models, which were used to generate findings and impressions of BUS reports from multi-modality and multi-view US images. (b) Similarity measurement between AI-generated reports and radiologist-written reports. (c) Malignancy risk assessment for each BI-RADS level in AI-generated reports. (d) Three senior radiologists evaluated reports generated by the AI system and reports written by one mid-level radiologist to determine the diagnostic level of AI-generated reports. (e) Three senior radiologists evaluated reports of junior radiologists with and without AI assistance to determine the value of the AI system to radiologists.


## train
To train the finding model, please use the following command:
```
python train_cnn.py  -image_folder data/image   -train_path  data/split/train.json  -val_path data/split/val.json
```

To train the impression model, please use the following command:
```
python train_cat.py  -image_folder data/image   -train_path  data/split/train.json  -val_path data/split/val.json
```

## inference
To test the finding model, please use the following command:
```
python translate_cnn.py  -image_folder data/image   -test_path  data/split/test.json -model your_finding_model_path.pth
```

To test the impression model, please use the following command:
```
python translate_cat.py  -image_folder data/image   -test_path  data/split/test.json -model your_impression_model_path.pth
```

## demo
Online [Demo](http://www.ai4busrg.com/) of automatically generated BUS reports and expert evaluation.

## preprocessing
A Chinese sentence is divided into multiple words using the following function:
```
def chinese_split(chinese_str):
    seg_list = jieba.cut(chinese_str, cut_all=False)
    seg_str = " ".join(seg_list)
    return seg_str
```
To use the above function, you need to install jieba first：
```
pip install jieba
```
## Acknowledgement
We implemented our transformer model with reference to [jadore801120](https://github.com/jadore801120/attention-is-all-you-need-pytorch/tree/master) and implemented the cider metric with reference to [brandontrabucco](https://github.com/brandontrabucco/cider). We thank them for their open source codes.
